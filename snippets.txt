max_length = 128

pretrain_x = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in pretrain['text']])
pretrain_y = torch.tensor(np.array(pretrain['label'].tolist()), dtype=torch.float32)

train_x = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in train['text']])
train_y = torch.tensor(np.array(train['label'].tolist()), dtype=torch.float32)

test_x = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in test['text']])
test_y = torch.tensor(np.array(test['label'].tolist()), dtype=torch.float32)

print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, pretrain_x.shape, pretrain_y.shape)

from torch.utils.data import Dataset
from torch import tensor
class CustomDataset(Dataset):
    def __init__(self, input_ids, label):
        self.input_ids = input_ids
        self.label = label

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'label': self.label[idx],
        }

pretrain_dataset = CustomDataset(pretrain_x, pretrain_y)
train_dataset = CustomDataset(train_x, train_y)
val_dataset = CustomDataset(test_x, test_y)





class EpochAverageLossCallback(TrainerCallback):
    def __init__(self):
        self.cumulative_loss = 0.0
        self.batch_count = 0

    def on_train_begin(self, args, state, control, **kwargs):
        # Reset at the beginning of training
        self.cumulative_loss = 0.0
        self.batch_count = 0

    def on_step_end(self, args, state, control, **kwargs):
        # Accumulate losses and increment batch count after each step
        self.cumulative_loss += state.log_history[-1]['loss'] if state.log_history else 0
        self.batch_count += 1

    def on_epoch_end(self, args, state, control, **kwargs):
        # Compute average loss and print it
        average_epoch_loss = self.cumulative_loss / self.batch_count
        print(f"Average training loss over epoch {state.epoch}: {average_epoch_loss:.4f}")

        # Reset for next epoch
        self.cumulative_loss = 0.0
        self.batch_count = 0


class PrintLearningRateCallback(TrainerCallback):
    def on_epoch_end(self, args: TrainingArguments, state: TrainerControl, control: TrainerControl, **kwargs):
        lr = state.log_history[-1]['learning_rate']
        print(f"Learning rate at end of epoch {state.epoch}: {lr}")



        # callbacks=[PrintLearningRateCallback(), EpochAverageLossCallback()]


























from datasets import load_metric

def compute_metrics(p):
    metric = load_metric("accuracy")
    metric_f1 = load_metric("f1")
    accuracy = metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)
    f1 = metric_f1.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='macro')
    return {"accuracy": accuracy["accuracy"], "f1": f1}


from torch.optim.lr_scheduler import LambdaLR
class EpochBasedLRScheduler(LambdaLR):
    def __init__(self, optimizer, lr_per_epoch, len_train_loader, last_epoch=-1):
        self.lr_per_epoch = lr_per_epoch
        self.len_train_loader = len_train_loader
        super(EpochBasedLRScheduler, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)

    def lr_lambda(self, current_step: int):
        current_epoch = current_step // self.len_train_loader
        if current_epoch < len(self.lr_per_epoch):
            return self.lr_per_epoch[current_epoch] / self.base_lrs[0]
        return 1.0



optimizer = AdamW(model.parameters(), lr=1e-5)
def train(model, lr_per_epoch, train_dataset):
    global optimizer, val_dataset

    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=len(lr_per_epoch),
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=0,
        weight_decay=0.01,
        logging_steps=len(train_dataset) // 4,
        #lr_scheduler_type='constant',
        #learning_rate=1e-9,
        report_to='none',
        evaluation_strategy='epoch',
        save_strategy='no',
    )

    len_train_loader = len(train_dataset) // training_args.per_device_train_batch_size
    scheduler = EpochBasedLRScheduler(optimizer, lr_per_epoch, len_train_loader)

    model.train()
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        optimizers=(optimizer, scheduler),
    )
    trainer.train()
    model.eval()



train(model, [1e-5,1e-5,1e-5], pretrain_dataset)
train(model, [1e-5,1e-5,1e-5], train_dataset)
torch.save(model, '/kaggle/working/model.pt' if kaggle else 'model.pt')
    

    # model = torch.load('/kaggle/working/model.pt' if kaggle else 'model.pt')
    # model.eval()









from datasets import load_metric
# Function to compute metrics
def compute_metrics(predictions, labels):
    metric = load_metric("accuracy")
    metric_f1 = load_metric("f1")
    accuracy = metric.compute(predictions=predictions.argmax(-1), references=labels)
    f1 = metric_f1.compute(predictions=predictions.argmax(-1), references=labels, average='macro')
    return {"accuracy": accuracy["accuracy"], "f1": f1}
	
	
	
	
	
	
	
	
	
	
	
	
	
new_layers_to_add = 3

from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaLayer

# Get the configuration of one existing layer to use as a template
template_config = model.config

# Manually create new layers using the template configuration
new_layers = [XLMRobertaLayer(template_config) for _ in range(new_layers_to_add)]

# Initialize the new layers with random weights
for layer in new_layers:
    layer.apply(model._init_weights)

# Append new layers to the existing stack of layers
model.roberta.encoder.layer.extend(new_layers)

# Update the config to reflect the new number of layers
model.config.num_hidden_layers += new_layers_to_add

# Freeze the original layers
for param in model.roberta.encoder.layer[:-new_layers_to_add].parameters():
    param.requires_grad = False