max_length = 128

pretrain_x = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in pretrain['text']])
pretrain_y = torch.tensor(np.array(pretrain['label'].tolist()), dtype=torch.float32)

train_x = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in train['text']])
train_y = torch.tensor(np.array(train['label'].tolist()), dtype=torch.float32)

test_x = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in test['text']])
test_y = torch.tensor(np.array(test['label'].tolist()), dtype=torch.float32)

print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, pretrain_x.shape, pretrain_y.shape)

from torch.utils.data import Dataset
from torch import tensor
class CustomDataset(Dataset):
    def __init__(self, input_ids, label):
        self.input_ids = input_ids
        self.label = label

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'label': self.label[idx],
        }

pretrain_dataset = CustomDataset(pretrain_x, pretrain_y)
train_dataset = CustomDataset(train_x, train_y)
val_dataset = CustomDataset(test_x, test_y)





class EpochAverageLossCallback(TrainerCallback):
    def __init__(self):
        self.cumulative_loss = 0.0
        self.batch_count = 0

    def on_train_begin(self, args, state, control, **kwargs):
        # Reset at the beginning of training
        self.cumulative_loss = 0.0
        self.batch_count = 0

    def on_step_end(self, args, state, control, **kwargs):
        # Accumulate losses and increment batch count after each step
        self.cumulative_loss += state.log_history[-1]['loss'] if state.log_history else 0
        self.batch_count += 1

    def on_epoch_end(self, args, state, control, **kwargs):
        # Compute average loss and print it
        average_epoch_loss = self.cumulative_loss / self.batch_count
        print(f"Average training loss over epoch {state.epoch}: {average_epoch_loss:.4f}")

        # Reset for next epoch
        self.cumulative_loss = 0.0
        self.batch_count = 0


class PrintLearningRateCallback(TrainerCallback):
    def on_epoch_end(self, args: TrainingArguments, state: TrainerControl, control: TrainerControl, **kwargs):
        lr = state.log_history[-1]['learning_rate']
        print(f"Learning rate at end of epoch {state.epoch}: {lr}")



        # callbacks=[PrintLearningRateCallback(), EpochAverageLossCallback()]



