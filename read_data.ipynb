{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "german_datasets = pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "german_datasets = german_datasets.astype({\"text\": str, \"label\": np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = pd.read_csv(\"data/german/news/RP-Mod.csv\")\n",
    "news1\n",
    "german_datasets = pd.concat([german_datasets, news1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2 = pd.read_csv(\"data/german/news/RP-Crowd-1.csv\")\n",
    "news2\n",
    "german_datasets = pd.concat([german_datasets, news1[[\"text\", \"label\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refugee = pd.read_csv(\"data/german/refugee/german hatespeech refugees.csv\")\n",
    "refugee[\"text\"] = refugee[\"Tweet\"]\n",
    "refugee[\"label\"] = (refugee[\"Hatespeech Rating (Expert 2)\"]-1)/5\n",
    "refugee\n",
    "german_datasets = pd.concat([german_datasets, refugee[[\"text\", \"label\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv(\"data/german/foreigners/comments.csv\")\n",
    "annotated_comments_df = pd.read_csv(\"data/german/foreigners/annotated_comments.csv\")\n",
    "\n",
    "# Group by 'comment_id' and calculate the mean valence for each 'comment_id'\n",
    "grouped_df = annotated_comments_df.groupby(\"comment_id\")[\"valence\"].mean().reset_index()\n",
    "\n",
    "# Merge the dataframes on 'comment_id'\n",
    "final_df = pd.merge(comments_df, grouped_df, on=\"comment_id\", how=\"inner\")\n",
    "\n",
    "# Rename columns to 'text' and 'label'\n",
    "final_df = final_df.rename(columns={\"message\": \"text\", \"valence\": \"label\"})\n",
    "\n",
    "# keep only the columns 'text' and 'label'\n",
    "final_df = final_df[[\"text\", \"label\"]]\n",
    "\n",
    "final_df[\"label\"] = final_df[\"label\"]-1\n",
    "\n",
    "german_datasets = pd.concat([german_datasets, final_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasoc = pd.read_csv(\"data/german/hasoc/german_dataset.tsv\", sep=\"\\t\")\n",
    "# hasoc[\"task_1\"] is always either NOT or HOF\n",
    "hasoc[\"label\"] = hasoc[\"task_1\"].map({\"NOT\": 0, \"HOF\": 1})\n",
    "\n",
    "german_datasets = pd.concat([german_datasets, hasoc[[\"text\", \"label\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germeval2018 = pd.read_csv(\"data/german/germeval2018/germeval2018.training.txt\", sep=\"\\t\", header=None)\n",
    "germeval2018.columns = [\"text\", \"label\", \"label2\"]\n",
    "germeval2018[\"label\"] = germeval2018[\"label\"].map({\"OTHER\": 0, \"OFFENSE\": 1})\n",
    "germeval2018[\"origin\"] = \"germeval\"\n",
    "\n",
    "german_datasets = pd.concat([german_datasets, germeval2018[[\"text\", \"label\", \"origin\"]]], ignore_index=True)\n",
    "\n",
    "germeval2018 = pd.read_csv(\"data/german/germeval2018/germeval2018.test.txt\", sep=\"\\t\", header=None)\n",
    "germeval2018.columns = [\"text\", \"label\", \"label2\"]\n",
    "germeval2018[\"label\"] = germeval2018[\"label\"].map({\"OTHER\": 0, \"OFFENSE\": 1})\n",
    "germeval2018[\"origin\"] = \"germeval\"\n",
    "\n",
    "german_datasets = pd.concat([german_datasets, germeval2018[[\"text\", \"label\", \"origin\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germeval2019 = pd.read_csv(\"data/german/germeval2019/Shared-Task-2019_Data_germeval2019.training_subtask1_2.txt\", sep=\"\\t\", header=None)\n",
    "germeval2019.columns = [\"text\", \"label\", \"label2\"]\n",
    "germeval2019[\"label\"] = germeval2019[\"label\"].map({\"OTHER\": 0, \"OFFENSE\": 1})\n",
    "germeval2019[\"origin\"] = \"germeval\"\n",
    "\n",
    "german_datasets = pd.concat([german_datasets, germeval2019[[\"text\", \"label\", \"origin\"]]], ignore_index=True)\n",
    "\n",
    "germeval2019 = pd.read_csv(\"data/german/germeval2019/fz.h-da.de_fileadmin_user_upload_germeval2019GoldLabelsSubtask1_2.txt\", sep=\"\\t\", header=None)\n",
    "germeval2019.columns = [\"text\", \"label\", \"label2\"]\n",
    "germeval2019[\"label\"] = germeval2019[\"label\"].map({\"OTHER\": 0, \"OFFENSE\": 1})\n",
    "germeval2019[\"origin\"] = \"germeval\"\n",
    "\n",
    "german_datasets = pd.concat([german_datasets, germeval2019[[\"text\", \"label\", \"origin\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated text\n",
    "german_datasets = german_datasets.drop_duplicates(subset=[\"text\"])\n",
    "german_datasets = german_datasets.reset_index(drop=True)\n",
    "\n",
    "#remove nan text\n",
    "german_datasets = german_datasets.dropna(subset=[\"text\"])\n",
    "german_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Cleans text into a basic form for NLP. Operations include the following:-\n",
    "    1. Remove special charecters like &, #, etc\n",
    "    2. Removes extra spaces\n",
    "    3. Removes embedded URL links\n",
    "    4. Removes HTML tags\n",
    "    5. Removes emojis\n",
    "    \n",
    "    text - Text piece to be cleaned.\n",
    "    '''\n",
    "    # print(text)\n",
    "    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n",
    "    text = template.sub(r'', text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n",
    "    only_text = soup.get_text()\n",
    "    text = only_text\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n",
    "    text = text.strip() # remove spaces at the beginning and at the end of string\n",
    "\n",
    "    return text\n",
    "\n",
    "german_datasets[\"text\"] = german_datasets[\"text\"].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "german_datasets = german_datasets.sample(frac=1).reset_index(drop=True)\n",
    "german_datasets.to_parquet(\"german_datasets.parquet\")\n",
    "\n",
    "#keep only texts with length above 5\n",
    "mock = german_datasets[german_datasets[\"text\"].str.len() > 5]\n",
    "mock_test_set = mock[:2000]\n",
    "mock_test_set = mock_test_set[[\"text\"]]\n",
    "mock_test_set.to_csv(\"mock_test_set.csv\", index=False)\n",
    "\n",
    "mock_test_labels = mock[:2000]\n",
    "mock_test_labels = mock_test_labels[[\"label\"]]\n",
    "mock_test_labels.to_csv(\"mock_test_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germeval = german_datasets[german_datasets[\"origin\"] == \"germeval\"]\n",
    "pretrain = german_datasets[german_datasets[\"origin\"] != \"germeval\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(germeval[\"label\"].value_counts())\n",
    "print(pretrain[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(germeval, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the files to parquet\n",
    "train.to_parquet(\"train.parquet\")\n",
    "test.to_parquet(\"test.parquet\")\n",
    "pretrain.to_parquet(\"pretrain.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (id, row) in train.iterrows():\n",
    "    print(row[\"text\"])\n",
    "    print(row[\"label\"])\n",
    "    print(\"----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
