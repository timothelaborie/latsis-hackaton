{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "kaggle = cwd == \"/kaggle/working\"\n",
    "\n",
    "# pretrain = pd.read_parquet(\"pretrain.parquet\")\n",
    "# train = pd.read_parquet(\"train.parquet\")\n",
    "# test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "pretrain = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"pretrain.parquet\")\n",
    "train = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"train.parquet\")\n",
    "test = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"test.parquet\")\n",
    "\n",
    "#convert to string\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "test[\"text\"] = test[\"text\"].astype(str)\n",
    "\n",
    "#keep only the first 1000 rows\n",
    "pretrain = pretrain[:1000]\n",
    "train = train[:1000]\n",
    "test = test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116_nocublaslt.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116_nocublaslt.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: C:\\ProgramData\\Anaconda3 did not contain ['cudart64_110.dll', 'cudart64_120.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('E:/texlive/2022/bin/win32'), WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin'), WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp'), WindowsPath('C:/ProgramData/Anaconda3/Library/usr/bin'), WindowsPath('C:/Program Files/NVIDIA/CUDNN/v8.x'), WindowsPath('C:/Users/Timothe/.dotnet/tools')}\n",
      "  warn(msg)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, XLMRobertaTokenizerFast, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "config = model.config\n",
    "tokenizer.model_max_length = config.max_position_embeddings\n",
    "model.classifier.out_proj = nn.Sequential(\n",
    "    nn.Linear(config.hidden_size, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/mdeberta-v3-base\", num_labels=1, ignore_mismatched_sizes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param in model.roberta.encoder.layer[-1].parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model.roberta.encoder.layer[-2].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaLayer\n",
    "\n",
    "# Get the configuration of one existing layer to use as a template\n",
    "template_config = model.config\n",
    "\n",
    "# Manually create new layers using the template configuration\n",
    "new_layers = [XLMRobertaLayer(template_config) for _ in range(2)]\n",
    "\n",
    "# Initialize the new layers with random weights\n",
    "for layer in new_layers:\n",
    "    layer.apply(model._init_weights)\n",
    "\n",
    "# Append new layers to the existing stack of layers\n",
    "model.roberta.encoder.layer.extend(new_layers)\n",
    "\n",
    "# Update the config to reflect the new number of layers\n",
    "model.config.num_hidden_layers += 2\n",
    "\n",
    "# Freeze the original layers\n",
    "for param in model.roberta.encoder.layer[:-2].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Check the architecture (should now have 14 layers in the encoder)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000]) torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000]) torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def encode_texts(tokenizer, texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "pretrain_x, pretrain_attention_mask = encode_texts(tokenizer, pretrain['text'])\n",
    "pretrain_y = torch.tensor(np.array(pretrain['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "train_x, train_attention_mask = encode_texts(tokenizer, train['text'])\n",
    "train_y = torch.tensor(np.array(train['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "test_x, test_attention_mask = encode_texts(tokenizer, test['text'])\n",
    "test_y = torch.tensor(np.array(test['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'label': self.label[idx],\n",
    "        }\n",
    "\n",
    "pretrain_dataset = CustomDataset(pretrain_x, pretrain_attention_mask, pretrain_y)\n",
    "train_dataset = CustomDataset(train_x, train_attention_mask, train_y)\n",
    "val_dataset = CustomDataset(test_x, test_attention_mask, test_y)\n",
    "\n",
    "print(train_x.shape, train_attention_mask.shape, train_y.shape, test_x.shape, test_attention_mask.shape, test_y.shape, pretrain_x.shape, pretrain_attention_mask.shape, pretrain_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbab398ec834694b2447cf2a8704dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2218, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760efda8acb04d26870de4f3610d9813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothe\\AppData\\Local\\Temp/ipykernel_30296/1550268740.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23930753767490387, 'eval_accuracy': 0.666, 'eval_f1': {'f1': 0.3997599039615847}, 'eval_runtime': 9.8188, 'eval_samples_per_second': 101.845, 'eval_steps_per_second': 25.461, 'epoch': 1.0}\n",
      "{'loss': 0.2204, 'learning_rate': 1e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b734f6842e4723b54147b85b138d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2321384847164154, 'eval_accuracy': 0.666, 'eval_f1': {'f1': 0.3997599039615847}, 'eval_runtime': 10.1669, 'eval_samples_per_second': 98.358, 'eval_steps_per_second': 24.589, 'epoch': 2.0}\n",
      "{'train_runtime': 123.6125, 'train_samples_per_second': 16.18, 'train_steps_per_second': 4.045, 'train_loss': 0.22110443878173827, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    accuracy = metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)\n",
    "    f1 = metric_f1.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='macro')\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1}\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "class EpochBasedLRScheduler(LambdaLR):\n",
    "    def __init__(self, optimizer, lr_per_epoch, len_train_loader, last_epoch=-1):\n",
    "        self.lr_per_epoch = lr_per_epoch\n",
    "        self.len_train_loader = len_train_loader\n",
    "        super(EpochBasedLRScheduler, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, current_step: int):\n",
    "        current_epoch = current_step // self.len_train_loader\n",
    "        if current_epoch < len(self.lr_per_epoch):\n",
    "            return self.lr_per_epoch[current_epoch] / self.base_lrs[0]\n",
    "        return 1.0\n",
    "\n",
    "    \n",
    "from transformers import TrainerCallback, TrainerControl, TrainingArguments\n",
    "\n",
    "class PrintLearningRateCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args: TrainingArguments, state: TrainerControl, control: TrainerControl, **kwargs):\n",
    "        lr = state.log_history[-1]['learning_rate']\n",
    "        print(f\"Learning rate at end of epoch {state.epoch}: {lr}\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "class EpochAverageLossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.cumulative_loss = 0.0\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        # Reset at the beginning of training\n",
    "        self.cumulative_loss = 0.0\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Accumulate losses and increment batch count after each step\n",
    "        self.cumulative_loss += state.log_history[-1]['loss'] if state.log_history else 0\n",
    "        self.batch_count += 1\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Compute average loss and print it\n",
    "        average_epoch_loss = self.cumulative_loss / self.batch_count\n",
    "        print(f\"Average training loss over epoch {state.epoch}: {average_epoch_loss:.4f}\")\n",
    "\n",
    "        # Reset for next epoch\n",
    "        self.cumulative_loss = 0.0\n",
    "        self.batch_count = 0\n",
    "\n",
    "\n",
    "lr_per_epoch = [1e-5,1e-5,1e-5]\n",
    "\n",
    "retrain = True\n",
    "# retrain = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=len(lr_per_epoch),\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=len(train_dataset) // 4,\n",
    "    #lr_scheduler_type='constant',\n",
    "    #learning_rate=1e-9,\n",
    "    report_to='none',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    ")\n",
    "\n",
    "len_train_loader = len(train_dataset) // training_args.per_device_train_batch_size\n",
    "\n",
    "\n",
    "\n",
    "def train(model, scheduler, train_dataset):\n",
    "    global optimizer, training_args, val_dataset\n",
    "    model.train()\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        # callbacks=[PrintLearningRateCallback(), EpochAverageLossCallback()]\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.eval()\n",
    "\n",
    "if retrain:\n",
    "    scheduler = EpochBasedLRScheduler(optimizer, lr_per_epoch, len_train_loader)\n",
    "    train(model, scheduler, pretrain_dataset)\n",
    "    scheduler = EpochBasedLRScheduler(optimizer, lr_per_epoch, len_train_loader)\n",
    "    train(model, scheduler, train_dataset)\n",
    "    torch.save(model, '/kaggle/working/model.pt' if kaggle else 'model.pt')\n",
    "    \n",
    "else:\n",
    "    model = torch.load('/kaggle/working/model.pt' if kaggle else 'model.pt')\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
