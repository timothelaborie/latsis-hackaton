{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "kaggle = cwd == \"/kaggle/working\"\n",
    "\n",
    "# pretrain = pd.read_parquet(\"pretrain.parquet\")\n",
    "# train = pd.read_parquet(\"train.parquet\")\n",
    "# test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "pretrain = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"pretrain.parquet\")\n",
    "train = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"train.parquet\")\n",
    "test = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"test.parquet\")\n",
    "\n",
    "#convert to string\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "test[\"text\"] = test[\"text\"].astype(str)\n",
    "\n",
    "#keep only the first 1000 rows\n",
    "# pretrain = pretrain[:1000]\n",
    "# train = train[:1000]\n",
    "test = test[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116_nocublaslt.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116_nocublaslt.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: C:\\ProgramData\\Anaconda3 did not contain ['cudart64_110.dll', 'cudart64_120.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Program Files/NVIDIA/CUDNN/v8.x'), WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin'), WindowsPath('C:/Users/Timothe/.dotnet/tools'), WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp'), WindowsPath('C:/ProgramData/Anaconda3/Library/usr/bin'), WindowsPath('E:/texlive/2022/bin/win32')}\n",
      "  warn(msg)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, XLMRobertaTokenizerFast, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup, TrainerCallback, TrainerControl, TrainingArguments\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "config = model.config\n",
    "tokenizer.model_max_length = config.max_position_embeddings\n",
    "# model.classifier.out_proj = nn.Sequential(\n",
    "#     nn.Linear(config.hidden_size, 1),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/mdeberta-v3-base\", num_labels=1, ignore_mismatched_sizes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.roberta.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.roberta.encoder.layer[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# new_layers_to_add = 4\n",
    "\n",
    "# from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaLayer\n",
    "\n",
    "# # Get the configuration of one existing layer to use as a template\n",
    "# template_config = model.config\n",
    "\n",
    "# # Manually create new layers using the template configuration\n",
    "# new_layers = [XLMRobertaLayer(template_config) for _ in range(new_layers_to_add)]\n",
    "\n",
    "# # Initialize the new layers with random weights\n",
    "# for layer in new_layers:\n",
    "#     layer.apply(model._init_weights)\n",
    "\n",
    "# # Append new layers to the existing stack of layers\n",
    "# model.roberta.encoder.layer.extend(new_layers)\n",
    "\n",
    "# # Update the config to reflect the new number of layers\n",
    "# model.config.num_hidden_layers += new_layers_to_add\n",
    "\n",
    "# # Freeze the original layers\n",
    "# for param in model.roberta.encoder.layer[:-new_layers_to_add].parameters():\n",
    "#     param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000]) torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000]) torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def encode_texts(tokenizer, texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "pretrain_x, pretrain_attention_mask = encode_texts(tokenizer, pretrain['text'])\n",
    "pretrain_y = torch.tensor(np.array(pretrain['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "train_x, train_attention_mask = encode_texts(tokenizer, train['text'])\n",
    "train_y = torch.tensor(np.array(train['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "test_x, test_attention_mask = encode_texts(tokenizer, test['text'])\n",
    "test_y = torch.tensor(np.array(test['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'label': self.label[idx],\n",
    "        }\n",
    "\n",
    "pretrain_dataset = CustomDataset(pretrain_x, pretrain_attention_mask, pretrain_y)\n",
    "train_dataset = CustomDataset(train_x, train_attention_mask, train_y)\n",
    "val_dataset = CustomDataset(test_x, test_attention_mask, test_y)\n",
    "\n",
    "print(train_x.shape, train_attention_mask.shape, train_y.shape, test_x.shape, test_attention_mask.shape, test_y.shape, pretrain_x.shape, pretrain_attention_mask.shape, pretrain_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████| 250/250 [00:11<00:00, 21.56it/s]\n",
      "Validation Epoch 1: 100%|██████████| 250/250 [00:07<00:00, 34.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.24551612832397224, Val Loss: 0.2147343755364418, Accuracy: 0.319, F1: 0.24184988627748297\n",
      "First predictions:\n",
      "y_pred: 0.5706850290298462, y: 0.0\n",
      "y_pred: 0.5708164572715759, y: 0.0\n",
      "y_pred: 0.5804279446601868, y: 1.0\n",
      "y_pred: 0.5784091949462891, y: 1.0\n",
      "y_pred: 0.5702053308486938, y: 0.0\n",
      "y_pred: 0.5796269178390503, y: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 250/250 [00:10<00:00, 24.32it/s]\n",
      "Validation Epoch 2: 100%|██████████| 250/250 [00:07<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2351092070043087, Val Loss: 0.1894984411597252, Accuracy: 0.32, F1: 0.24349965957484682\n",
      "First predictions:\n",
      "y_pred: 0.580420970916748, y: 0.0\n",
      "y_pred: 0.5842334032058716, y: 0.0\n",
      "y_pred: 0.6191871166229248, y: 1.0\n",
      "y_pred: 0.6448922157287598, y: 1.0\n",
      "y_pred: 0.565186083316803, y: 0.0\n",
      "y_pred: 0.5924152135848999, y: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 250/250 [00:10<00:00, 23.56it/s]\n",
      "Validation Epoch 3: 100%|██████████| 250/250 [00:07<00:00, 32.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1987261294648051, Val Loss: 0.17340797478333114, Accuracy: 0.425, F1: 0.400286402354632\n",
      "First predictions:\n",
      "y_pred: 0.5854322910308838, y: 0.0\n",
      "y_pred: 0.5414919257164001, y: 0.0\n",
      "y_pred: 0.5999466180801392, y: 1.0\n",
      "y_pred: 0.6574256420135498, y: 1.0\n",
      "y_pred: 0.5159902572631836, y: 0.0\n",
      "y_pred: 0.5610872507095337, y: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 250/250 [00:10<00:00, 23.28it/s]\n",
      "Validation Epoch 4: 100%|██████████| 250/250 [00:07<00:00, 32.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.18926334313303234, Val Loss: 0.17550691456347703, Accuracy: 0.345, F1: 0.2844024870017994\n",
      "First predictions:\n",
      "y_pred: 0.5860961079597473, y: 0.0\n",
      "y_pred: 0.5582557916641235, y: 0.0\n",
      "y_pred: 0.6846325993537903, y: 1.0\n",
      "y_pred: 0.6829226016998291, y: 1.0\n",
      "y_pred: 0.5538344383239746, y: 0.0\n",
      "y_pred: 0.6017970442771912, y: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 250/250 [00:10<00:00, 23.29it/s]\n",
      "Validation Epoch 5: 100%|██████████| 250/250 [00:07<00:00, 32.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1765116575062275, Val Loss: 0.16242653753701597, Accuracy: 0.452, F1: 0.4372306820189041\n",
      "First predictions:\n",
      "y_pred: 0.6013263463973999, y: 0.0\n",
      "y_pred: 0.49808311462402344, y: 0.0\n",
      "y_pred: 0.6444921493530273, y: 1.0\n",
      "y_pred: 0.6954272985458374, y: 1.0\n",
      "y_pred: 0.49415677785873413, y: 0.0\n",
      "y_pred: 0.5850968360900879, y: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  64%|██████▎   | 159/250 [00:06<00:04, 20.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  78%|███████▊  | 195/250 [00:08<00:02, 22.80it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = AdamW(model.parameters(), lr=999)\n",
    "\n",
    "# Training function\n",
    "def train(model, lr_per_epoch, train_dataset, val_dataset):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    for epoch in range(len(lr_per_epoch)):\n",
    "        model.train()\n",
    "        lr = lr_per_epoch[epoch]\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Training loop with tqdm\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            inputs = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            labels = batch['label'].cuda()\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_predictions_raw = []\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Validation loop with tqdm\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            inputs = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            labels = batch['label'].cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs.logits\n",
    "                val_loss += outputs.loss.item()\n",
    "                pred = F.sigmoid(logits)\n",
    "                \n",
    "                all_predictions_raw.append(pred.cpu())\n",
    "                all_predictions.append(torch.round(pred).cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "        all_predictions_raw = torch.cat(all_predictions_raw)\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        accuracy = accuracy_score(all_labels.numpy(), all_predictions.numpy())\n",
    "        f1 = f1_score(all_labels.numpy(), all_predictions.numpy(), average='macro')\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}, Accuracy: {accuracy}, F1: {f1}\")\n",
    "\n",
    "        print(f\"First predictions:\")\n",
    "        i = 0\n",
    "        for y_pred, y in zip(all_predictions_raw, all_labels):\n",
    "            print(f\"y_pred: {y_pred.item()}, y: {y}\")\n",
    "            i += 1\n",
    "            if i > 5:\n",
    "                break\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Train the model\n",
    "# train(model, [1e-5, 1e-5, 1e-5], pretrain_dataset, val_dataset)\n",
    "train(model, [1e-9, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5], train_dataset, val_dataset)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '/kaggle/working/model.pt' if 'kaggle' in globals() else 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
