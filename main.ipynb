{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "kaggle = cwd == \"/kaggle/working\"\n",
    "\n",
    "# pretrain = pd.read_parquet(\"pretrain.parquet\")\n",
    "# train = pd.read_parquet(\"train.parquet\")\n",
    "# test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "pretrain = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"pretrain.parquet\")\n",
    "train = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"train.parquet\")\n",
    "test = pd.read_parquet((\"/kaggle/input/latsis-experiments/\" if kaggle else \"\") + \"test.parquet\")\n",
    "\n",
    "#convert to string\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "test[\"text\"] = test[\"text\"].astype(str)\n",
    "\n",
    "#keep only the first 1000 rows\n",
    "pretrain = pretrain[:1000]\n",
    "train = train[:1000]\n",
    "test = test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116_nocublaslt.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116_nocublaslt.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: C:\\ProgramData\\Anaconda3 did not contain ['cudart64_110.dll', 'cudart64_120.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin'), WindowsPath('C:/Program Files/NVIDIA/CUDNN/v8.x'), WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp'), WindowsPath('C:/ProgramData/Anaconda3/Library/usr/bin'), WindowsPath('C:/Users/Timothe/.dotnet/tools'), WindowsPath('E:/texlive/2022/bin/win32')}\n",
      "  warn(msg)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, XLMRobertaTokenizerFast, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup, TrainerCallback, TrainerControl, TrainingArguments\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=1)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "config = model.config\n",
    "tokenizer.model_max_length = config.max_position_embeddings\n",
    "# model.classifier.out_proj = nn.Sequential(\n",
    "#     nn.Linear(config.hidden_size, 1),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/mdeberta-v3-base\", num_labels=1, ignore_mismatched_sizes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param in model.roberta.encoder.layer[-1].parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model.roberta.encoder.layer[-2].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaLayer\n",
    "\n",
    "# Get the configuration of one existing layer to use as a template\n",
    "template_config = model.config\n",
    "\n",
    "# Manually create new layers using the template configuration\n",
    "new_layers = [XLMRobertaLayer(template_config) for _ in range(2)]\n",
    "\n",
    "# Initialize the new layers with random weights\n",
    "for layer in new_layers:\n",
    "    layer.apply(model._init_weights)\n",
    "\n",
    "# Append new layers to the existing stack of layers\n",
    "model.roberta.encoder.layer.extend(new_layers)\n",
    "\n",
    "# Update the config to reflect the new number of layers\n",
    "model.config.num_hidden_layers += 2\n",
    "\n",
    "# Freeze the original layers\n",
    "for param in model.roberta.encoder.layer[:-2].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Check the architecture (should now have 14 layers in the encoder)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000]) torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000]) torch.Size([1000, 128]) torch.Size([1000, 128]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def encode_texts(tokenizer, texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "pretrain_x, pretrain_attention_mask = encode_texts(tokenizer, pretrain['text'])\n",
    "pretrain_y = torch.tensor(np.array(pretrain['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "train_x, train_attention_mask = encode_texts(tokenizer, train['text'])\n",
    "train_y = torch.tensor(np.array(train['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "test_x, test_attention_mask = encode_texts(tokenizer, test['text'])\n",
    "test_y = torch.tensor(np.array(test['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'label': self.label[idx],\n",
    "        }\n",
    "\n",
    "pretrain_dataset = CustomDataset(pretrain_x, pretrain_attention_mask, pretrain_y)\n",
    "train_dataset = CustomDataset(train_x, train_attention_mask, train_y)\n",
    "val_dataset = CustomDataset(test_x, test_attention_mask, test_y)\n",
    "\n",
    "print(train_x.shape, train_attention_mask.shape, train_y.shape, test_x.shape, test_attention_mask.shape, test_y.shape, pretrain_x.shape, pretrain_attention_mask.shape, pretrain_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Timothe\\AppData\\Local\\Temp/ipykernel_22496/2589682640.py:9: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - {'accuracy': 0.681, 'f1': {'f1': 0.405116002379536}}\n",
      "Epoch 2 - {'accuracy': 0.681, 'f1': {'f1': 0.405116002379536}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">64</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Epoch {</span>epoch+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">} - {</span>metrics<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">62 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">63 # Train the model</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>64 train(model, [<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-5</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-5</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-5</span>], pretrain_dataset, val_dataset)                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">65 </span>train(model, [<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-5</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-5</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-5</span>], train_dataset, val_dataset)                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">66 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">67 # Save the model</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>lr = lr_per_epoch[epoch]                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>optimizer.param_groups[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'lr'</span>] = lr                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> batch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> train_loader:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>32 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>inputs = batch[<span style=\"color: #808000; text-decoration-color: #808000\">'input_ids'</span>].cuda()                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>attention_mask = batch[<span style=\"color: #808000; text-decoration-color: #808000\">'attention_mask'</span>].cuda()                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>labels = batch[<span style=\"color: #808000; text-decoration-color: #808000\">'label'</span>].cuda()                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m64\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m61 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mEpoch \u001b[0m\u001b[33m{\u001b[0mepoch+\u001b[94m1\u001b[0m\u001b[33m}\u001b[0m\u001b[33m - \u001b[0m\u001b[33m{\u001b[0mmetrics\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m62 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m# Train the model\u001b[0m                                                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m64 train(model, [\u001b[94m1e-5\u001b[0m, \u001b[94m1e-5\u001b[0m, \u001b[94m1e-5\u001b[0m], pretrain_dataset, val_dataset)                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m65 \u001b[0mtrain(model, [\u001b[94m1e-5\u001b[0m, \u001b[94m1e-5\u001b[0m, \u001b[94m1e-5\u001b[0m], train_dataset, val_dataset)                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m66 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m67 \u001b[0m\u001b[2m# Save the model\u001b[0m                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92mtrain\u001b[0m:\u001b[94m32\u001b[0m                                                                                      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mlr = lr_per_epoch[epoch]                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0moptimizer.param_groups[\u001b[94m0\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mlr\u001b[0m\u001b[33m'\u001b[0m] = lr                                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mfor\u001b[0m batch \u001b[95min\u001b[0m train_loader:                                                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m32 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0minputs = batch[\u001b[33m'\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m'\u001b[0m].cuda()                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mattention_mask = batch[\u001b[33m'\u001b[0m\u001b[33mattention_mask\u001b[0m\u001b[33m'\u001b[0m].cuda()                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mlabels = batch[\u001b[33m'\u001b[0m\u001b[33mlabel\u001b[0m\u001b[33m'\u001b[0m].cuda()                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m35 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_metric\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(predictions, labels):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    accuracy = metric.compute(predictions=predictions.argmax(-1), references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions.argmax(-1), references=labels, average='macro')\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1}\n",
    "\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=999)\n",
    "model = model.cuda()\n",
    "\n",
    "# Training function\n",
    "def train(model, lr_per_epoch, train_dataset, val_dataset):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(len(lr_per_epoch)):\n",
    "        model.train()\n",
    "        lr = lr_per_epoch[epoch]\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            labels = batch['label'].cuda()\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['input_ids'].cuda()\n",
    "                attention_mask = batch['attention_mask'].cuda()\n",
    "                labels = batch['label'].cuda()\n",
    "\n",
    "                outputs = model(inputs, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                all_predictions.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        metrics = compute_metrics(all_predictions, all_labels)\n",
    "        print(f\"Epoch {epoch+1} - {metrics}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, [1e-5, 1e-5, 1e-5], pretrain_dataset, val_dataset)\n",
    "train(model, [1e-5, 1e-5, 1e-5], train_dataset, val_dataset)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '/kaggle/working/model.pt' if kaggle else 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
