{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing gelectra\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "[-4.737 -4.211  1.678  0.399 -5.08  -2.838  3.709 -2.921 -5.411 -2.713 -4.965 -2.886  0.254 -2.167  4.205 -0.42   2.191  0.325  3.467  2.771  0.568 -3.55   1.764 -3.771 -5.235  2.241 -0.777 -5.021\n",
      "  0.336  4.492  3.703  1.298 -2.384 -1.702  3.084  2.887 -1.151 -4.262  4.407 -3.581 -4.66  -0.599 -3.625 -2.847 -1.163  1.507 -5.051 -4.222  2.701 -5.096  4.615  1.967 -4.495 -5.217 -5.561  4.416\n",
      " -4.893  3.262  3.238 -5.329 -4.715 -5.307  4.121 -5.281 -1.727 -3.681 -2.165 -0.696 -2.444 -4.951 -5.009 -3.735 -2.716 -2.38  -1.271 -0.168  4.19  -4.325 -0.911 -5.389 -4.075 -5.117 -5.596  3.229\n",
      " -4.993 -4.439  2.546 -5.281  0.004 -2.911 -3.97  -5.13  -2.192  4.351  0.927 -4.798  0.963 -5.199 -2.332 -5.297 -2.562 -0.207 -5.347 -4.394  2.006 -5.048 -5.35   1.043 -4.915 -4.54   3.512  1.662\n",
      "  2.194  3.763 -4.435 -4.81   3.05  -4.858  2.017 -0.886 -4.213 -3.365 -5.183 -3.271 -5.2   -4.613 -3.479 -4.181 -3.799  3.731 -4.888 -3.181 -4.731  4.04  -4.337 -5.115 -4.446 -1.885 -4.073 -4.253\n",
      "  2.947  0.084 -1.665  4.662  4.621 -4.283 -3.539 -2.902 -5.309 -5.221 -4.597 -1.911 -5.415 -5.024  1.423 -3.285 -3.039  3.646 -2.832  4.084 -4.731  2.615  4.315 -5.057 -3.175  2.205 -5.    -2.615\n",
      " -3.93  -4.875  2.615  1.29   4.248  0.683  4.458 -4.729 -4.361  2.86  -0.44  -1.504 -2.358 -4.671  4.189  1.761 -0.797 -3.707  2.093 -2.586 -4.03  -5.235  0.56  -0.783 -1.707 -4.711  3.339  0.291\n",
      " -4.813 -2.246 -5.252  2.173 -5.223  4.699 -2.475  2.398 -3.049 -5.031 -4.871 -3.643 -5.053 -4.429  1.784 -5.409 -5.029 -3.253 -5.575 -4.443 -1.721 -4.254 -2.54   4.237 -5.274 -4.183 -1.491 -3.97\n",
      " -4.854  1.029 -5.27  -5.346 -1.366 -3.169 -4.808  2.329 -3.488  0.205 -3.549  3.528 -1.749  2.404  3.67  -5.353 -5.447  1.064  1.989 -0.766 -1.645 -5.615  2.134 -4.297  2.047 -4.491  2.323 -5.356\n",
      " -4.953 -4.78  -4.865 -5.356 -2.283 -3.368 -3.655 -2.003]\n",
      "processing mdebertav3\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "[-3.457 -3.743  0.55  -2.819 -4.592 -1.382  3.414  2.274 -4.465  1.606 -4.796  0.092 -3.134 -3.565 -2.033 -0.108  1.928  3.458  3.581  3.373  1.815 -2.452  3.173 -3.673 -4.503  0.653  2.584 -4.803\n",
      "  2.228  3.645  3.061  3.744 -1.189  2.476  2.322 -1.497  3.356 -4.211  3.678 -0.753 -4.766  0.691 -3.926  2.983 -2.031  2.372 -4.396 -3.22   2.963 -4.74   3.435  3.448 -4.435 -4.588 -4.57   3.69\n",
      " -3.855  2.601  3.193 -4.722 -4.491 -4.745 -0.468 -4.638 -3.341 -3.099  0.922  2.325 -0.65  -4.616 -4.781  0.735 -3.338 -1.943  3.268  2.308  3.223 -3.329 -1.982 -4.624  1.061 -4.546 -4.812  3.18\n",
      " -3.589 -0.511  1.693 -4.506  1.13  -1.919 -3.827 -4.817  2.923  3.616  2.569 -4.681  1.108 -2.876  1.531 -4.731 -0.022  2.187 -4.773 -2.272  1.1   -4.71  -4.771 -2.576 -3.881 -3.521  3.232  3.51\n",
      "  3.529  2.581 -4.665 -4.437  1.375 -2.369  1.274 -3.042 -1.067 -3.628 -4.518 -2.897 -4.15  -3.264 -3.725 -3.955 -4.124  3.505 -4.359 -1.054 -2.539  2.921 -4.748 -4.562 -4.149 -2.773 -2.262 -1.879\n",
      "  0.541 -0.218 -2.998  3.576 -3.096 -4.064 -3.049 -0.44  -4.772 -3.666 -2.686  2.25  -4.824 -4.525 -0.159 -2.55  -2.04   3.003  1.194  3.527 -2.724  1.491 -3.286 -4.767 -1.475  3.135 -4.68  -3.755\n",
      " -4.72  -4.761  2.695  0.427  3.586 -2.267  1.835 -4.187 -1.796  1.9   -4.177  2.127 -0.521 -3.262  2.173 -0.178  3.041 -4.446  1.029  0.182 -4.662 -4.765 -4.168  1.078 -1.126 -2.506  3.305 -0.774\n",
      " -4.465 -3.168 -4.499  0.327 -4.871  3.764  1.79   0.962  0.877 -4.675 -4.346 -3.868 -1.635 -3.463  0.092 -4.693 -4.835  1.845 -4.076  0.623  2.815  1.892  0.304  3.683 -4.481 -3.119 -4.71  -4.633\n",
      " -3.936  0.323 -4.778 -4.773 -1.313 -3.916 -4.86  -1.027 -2.498 -1.223 -1.846  2.424  3.086 -0.87   2.005 -4.686 -4.498  0.671 -4.212  3.327 -3.555 -4.616 -0.188 -3.39   3.008 -4.518  1.384 -4.549\n",
      " -4.675 -3.54  -2.545 -4.773 -2.094 -3.204 -2.544 -2.704]\n",
      "processing semantic\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "[-3.034 -6.41   2.634 -2.649 -6.52  -4.691  6.098  1.025 -6.7   -2.069 -6.94  -0.349 -4.814 -0.932  4.675 -2.273  0.459  4.501  6.185  2.579  2.259 -4.028  5.129 -3.001 -6.593 -0.574 -3.601 -6.453\n",
      "  1.58   6.189  2.388  4.202 -0.223  1.007  4.092  0.397  5.851 -6.305  5.734 -5.561 -5.458  3.091 -6.177  1.092 -4.426  5.685 -6.468 -6.436  4.562 -5.757  5.248 -0.223  0.026 -6.144 -0.7    5.598\n",
      " -4.806  5.49   6.216 -6.477 -6.518 -5.136  1.163 -6.255 -2.742 -5.439 -1.032  3.507  3.058 -6.011 -5.655  0.11  -2.658 -3.138  2.422  2.829  5.653 -5.685 -3.29  -6.142 -0.608 -5.426 -6.03   3.245\n",
      " -1.754 -6.225  4.185 -6.527  5.007 -5.617 -4.639 -6.043  0.638  6.457  2.367 -4.08  -1.534 -5.851  0.562 -6.694  2.602 -4.846 -6.903 -0.532  2.487 -4.79  -5.009 -6.479 -6.427 -4.126  2.558  4.399\n",
      "  5.614  4.415 -6.578 -6.591  3.227 -5.802  2.424 -2.903 -5.303 -3.399 -6.197 -4.135 -2.447 -3.854 -6.089 -5.397 -5.037  1.797 -2.483 -6.277 -6.243  4.717 -5.827 -6.295 -2.862 -5.607  0.441 -3.238\n",
      "  4.583 -1.21  -1.442  5.671  6.011 -6.434 -2.99   0.735 -6.049 -6.629 -5.551 -1.553 -5.931 -6.396 -2.833 -6.146 -4.388  5.2    4.608  6.112 -1.482  1.64   4.182 -6.466 -2.54   3.703 -6.752 -0.635\n",
      " -6.629 -6.605  5.212 -2.371  5.907  4.811  5.598 -6.759 -4.399  4.916 -4.729 -5.436  1.059 -6.356  5.326 -3.253  4.798 -5.57  -5.01  -5.924 -5.508 -6.49  -1.354  2.927 -1.686 -6.23   3.632 -3.832\n",
      " -6.416 -6.331 -4.53  -0.635 -6.653  6.368  3.189 -5.558  0.663 -6.732 -6.196 -6.179 -4.857 -2.996  0.841 -6.345 -6.729 -3.921 -6.619 -2.129 -5.78  -4.684 -0.393  6.048 -6.703 -6.222 -6.215 -6.28\n",
      " -6.543 -0.148 -5.936 -5.953 -2.587 -5.697 -6.166 -3.5   -5.188 -2.11  -5.482  3.817 -3.772 -6.033  5.51  -6.672 -4.147 -2.393 -5.739  2.279  0.672 -5.972  3.573  0.183 -2.327 -5.695 -0.501 -6.354\n",
      " -6.325 -6.595 -6.52  -6.54   4.298 -2.997 -4.541 -5.152]\n",
      "processing xlm_roberta_base\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "[-1.657 -2.62   0.593  2.409 -5.043 -2.095  4.179 -0.085 -5.042 -4.253 -5.181 -0.496 -2.322 -4.526 -3.492 -1.726 -1.391 -1.408  6.234  2.26   0.758 -0.316 -0.88  -4.602 -5.739  0.194  1.459 -5.295\n",
      "  1.992  5.997  1.325  1.767 -1.447  0.236  0.953 -1.167 -0.95  -4.594  4.855 -2.344 -5.002  4.542  0.578 -1.669 -2.342 -0.394 -5.147 -5.17  -1.136 -4.777  3.629  1.204 -4.437 -3.468 -4.998  4.355\n",
      " -2.925  3.872  0.94  -5.089 -6.015 -4.499 -0.476 -4.973 -0.079 -2.719 -1.697  0.839 -1.733 -4.028 -4.875 -0.152 -3.498 -1.673  2.757  2.459  1.786 -2.307  0.36  -3.348  0.617 -4.584 -4.608  0.96\n",
      " -4.471 -2.919 -1.074 -5.677  1.923 -2.192 -1.97  -5.333  1.713  3.188  0.304 -1.529 -1.964 -5.229  0.628 -6.206 -0.494  1.288 -5.54  -2.533 -2.093 -4.566 -5.51  -2.796 -5.242 -3.23   0.906  2.533\n",
      "  1.353 -0.636 -4.724 -4.862  1.878 -3.977  0.326 -1.158 -4.218 -1.691 -4.315 -0.455 -3.135 -3.394 -0.809 -1.09  -2.271  1.868 -5.001 -2.801 -4.249  1.272 -0.006 -4.474 -4.528 -2.585 -0.868 -3.511\n",
      "  1.531 -0.65  -0.287  5.678 -1.158 -5.214 -1.373 -2.805 -1.171 -3.951 -4.269 -0.359 -5.314 -4.971  1.172 -1.806  0.534  4.229 -1.525  2.55  -0.62   1.423 -0.492 -5.602 -4.926  3.134 -5.054 -0.944\n",
      " -4.824 -5.229  6.004 -0.659  5.149  1.948  4.207 -4.126 -2.736  1.22  -1.03  -1.937 -2.953 -5.78   3.401  0.042  2.045 -4.321 -0.877  0.637 -1.639 -5.324 -1.014  4.329  1.188 -1.257  2.984  0.247\n",
      " -4.482 -4.359 -1.981  2.161 -6.053  2.583  0.17  -1.573  1.713 -4.991 -4.973 -1.355 -4.755 -4.3   -0.482 -3.952 -7.004 -2.3   -1.185 -1.547 -3.796 -5.106 -2.051  1.908 -4.7   -3.42  -5.703 -5.039\n",
      " -3.977  4.644 -5.677 -5.221 -0.693 -2.155 -5.161  3.673 -3.043 -1.229 -3.026  1.886 -0.676 -4.491 -0.052 -5.362 -4.412 -3.082 -2.358  2.932 -2.292 -4.473  1.768  2.812  2.053 -5.189 -1.973 -5.73\n",
      " -5.528 -5.161 -4.191 -5.956 -3.181 -3.207 -2.11  -3.317]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc  # For garbage collection\n",
    "np.set_printoptions(suppress=True, precision=3, edgeitems=10, linewidth=200)\n",
    "\n",
    "test = pd.read_parquet(\"../test.parquet\")\n",
    "test = test[:260]\n",
    "test_y = torch.tensor(np.array(test['label'].tolist()), dtype=torch.float32)\n",
    "\n",
    "def encode_texts(tokenizer, texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "\n",
    "stuff = os.listdir(\"./\")\n",
    "for folder in stuff:\n",
    "    if folder.__contains__(\".\"):\n",
    "        continue\n",
    "\n",
    "    print(\"processing \" + folder)\n",
    "\n",
    "    model = torch.load(folder + \"/model.pt\").cuda()\n",
    "    tokenizer = torch.load(folder + \"/tokenizer.pt\")\n",
    "\n",
    "    test_x, test_attention_mask = encode_texts(tokenizer, test['text'])\n",
    "\n",
    "    BS = 5\n",
    "    predictions = []\n",
    "    for i in range(0, len(test_x), BS):\n",
    "        batch = test_x[i:i+BS].cuda()\n",
    "        batch_attention_mask = test_attention_mask[i:i+BS].cuda()\n",
    "\n",
    "        with torch.no_grad():  # Deactivate autograd engine to reduce memory usage\n",
    "            prediction = model(batch, batch_attention_mask).logits.squeeze(-1)\n",
    "        predictions.append(prediction.cpu().detach())  # Detach before moving to CPU\n",
    "\n",
    "        del batch\n",
    "        del batch_attention_mask\n",
    "        del prediction\n",
    "        torch.cuda.empty_cache()  # Free up memory\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    predictions = torch.cat(predictions)\n",
    "    predictions = predictions.detach().numpy()\n",
    "    print(predictions)\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del test_x\n",
    "    del test_attention_mask\n",
    "    torch.cuda.empty_cache()  # Free up memory\n",
    "    gc.collect()  # Trigger Python garbage collection\n",
    "\n",
    "    np.save(folder + \"/predictions.npy\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260,)\n"
     ]
    }
   ],
   "source": [
    "#average predictions\n",
    "all_predictions = []\n",
    "for folder in stuff:\n",
    "    if folder.__contains__(\".\"):\n",
    "        continue\n",
    "    predictions = np.load(folder + \"/predictions.npy\")\n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "all_predictions = np.mean(all_predictions, axis=0)\n",
    "print(all_predictions.shape)\n",
    "\n",
    "#save to a csv file\n",
    "df = pd.DataFrame(all_predictions, columns=[\"pred\"])\n",
    "df.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
